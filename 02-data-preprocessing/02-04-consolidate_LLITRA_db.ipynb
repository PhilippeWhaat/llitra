{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -.-.-.-.-.-. OBJECTIVE .-.-.-.-.-.-.-\n",
    "The objective of this program is to build a consolidated LIST_CONSOL database between: \n",
    "1. MetaDrive data from LIST_ARCH describing the files collected in the LLITRA collaboration between Digital iuris and BJGS\n",
    "2. The anonymized data from LIST_CONC constituting the textual material for identifying legal concepts\n",
    "\n",
    "##### -.-.-.-.-.-. How to run .-.-.-.-.-.-.-\n",
    "\n",
    "Step 1. Choose the folder where the files from Universal Data Tool containing the anonymized data are located to construct LIST_CONC\n",
    "\n",
    "Step 2. Choose the folder where the files collected through BJGS are located to construct MD5 identification and Path for LIST_ARCH\n",
    "\n",
    "Step 3. Select the CSV file holding the rest of LIST_ARCH data from MetaDrive\n",
    "\n",
    "##### -.-.-.-.-.-.-. PROCESS .-.-.-.-.-.-.-.-\n",
    "\n",
    "The data from Step 1 is first cleaned to be anonymized and a path is assigned to each record, which are stored in LIST_CONC.\n",
    "The data from Step 2 and 3 is then concatenated in LIST_ARCH. Each register of the anonymized database is completed by the Metadrive metadata according to the file from which it is extracted.\n",
    "\n",
    "LIST_ARCH is compared with LIST_CONC to assign the registers in the latter to the appropriate registers in the former, based on the path and file name from both lists.\n",
    "- If the record from LIST_CONC does not have metadrive data, or is not anonymized, then it is added to a LIST_CORR database pending correction.\n",
    "- If the record from LIST_CONC has a corresponding LIST_ARCH register, the result is stored in LIST_CONSOL.\n",
    "\n",
    "No manipulation is performed on the data indicated to the program: the use of the OS module is limited to data extraction. \n",
    "The transformation and load process are performed in memory before saving LIST_CONSOL and LIST_CORR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, hashlib, ntpath, json, re, csv\n",
    "import pandas as pd\n",
    "from tkinter import Tk, filedialog, messagebox\n",
    "##CUR_DIR = str(os.path.dirname(__file__))\n",
    "CUR_DIR = str(os.path.dirname(\"WRITE HERE THE CURRENT PATH WHERE YOU EXECUTE THE PROGRAM (PYTHON NOTEBOOK DOES NOT SUPPORT THE __file__ REFERENCE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get folders and file location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data(choosefileorfolder, boxTitle, boxDescription):\n",
    "    root = Tk() # pointing root to Tk() to use it as Tk() in program.\n",
    "    root.withdraw() # Hides small tkinter window.\n",
    "    root.attributes('-topmost', True) # Opened windows will be active. above all windows despite of selection.\n",
    "\n",
    "    messagebox.showinfo(boxTitle, boxDescription)\n",
    "    if choosefileorfolder == 1:\n",
    "        path_from_box = filedialog.askdirectory(initialdir=CUR_DIR) # Returns opened path as str\n",
    "    elif choosefileorfolder == 2:\n",
    "        path_from_box = filedialog.askopenfilename(initialdir=CUR_DIR) # Returns opened path as str\n",
    "    \n",
    "    if path_from_box == \"\":\n",
    "        exit()\n",
    "    return path_from_box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Choose the folder where the files from Universal Data Tool containing the anonymized data are located to construct LIST_CONC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 17:17:12.745 python[25340:478519] Warning: Expected min height of view: (<NSButton: 0x7fe9912db410>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-11-03 17:17:39.276 python[25340:478519] +[CATransaction synchronize] called within transaction\n",
      "2022-11-03 17:17:39.553 python[25340:478519] +[CATransaction synchronize] called within transaction\n"
     ]
    }
   ],
   "source": [
    "folder_UDT_Path = select_data(1, \"Folder with UDT files\", \"Please select the folder where the files from Universal Data Tool containing the anonymized data are located to construct LIST_CONC.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Choose the folder where the files collected through BJGS are located to construct MD5 identification and Path for LIST_ARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 17:18:21.436 python[25340:478519] Warning: Expected min height of view: (<NSButton: 0x7fe99261fce0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-11-03 17:18:32.034 python[25340:478519] +[CATransaction synchronize] called within transaction\n",
      "2022-11-03 17:18:32.133 python[25340:478519] +[CATransaction synchronize] called within transaction\n",
      "2022-11-03 17:19:00.511 python[25340:478519] +[CATransaction synchronize] called within transaction\n",
      "2022-11-03 17:19:36.599 python[25340:478519] +[CATransaction synchronize] called within transaction\n"
     ]
    }
   ],
   "source": [
    "folder_BJGS_RAW_Path = select_data(1, \"Folder with BJGS PDF and DOCX files\", \"Please select the folder where the files collected through BJGS are located to construct MD5 identification and Path for LIST_ARCH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Select the CSV file holding the rest of LIST_ARCH data from MetaDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 17:19:45.761 python[25340:478519] Warning: Expected min height of view: (<NSButton: 0x7fe9926331e0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-11-03 17:19:47.396 python[25340:478519] +[CATransaction synchronize] called within transaction\n",
      "2022-11-03 17:19:47.449 python[25340:478519] +[CATransaction synchronize] called within transaction\n"
     ]
    }
   ],
   "source": [
    "CSV_Metadrive_Path = select_data(2, \"CSV file with Metadrive Data\", \"Please select the CSV File where the Metadrive data is stored to populate the rest of LIST_ARCH data from MetaDrive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the panda dataframe from BJGS files with file_MD5, file_path, file_name and file_dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnp_dict(folder_path):\n",
    "    mnp_dict = {\"file_MD5\":[], \"file_name\":[], \"file_path\":[], \"file_dirname\":[]}\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith((\".doc\", \".docx\", \".pdf\")):\n",
    "                new_path_result = os.path.join(root, file)\n",
    "                new_name_result = ntpath.basename(new_path_result)\n",
    "                new_md5_result  = hashlib.md5(open(new_path_result,\"rb\").read()).hexdigest()\n",
    "                new_dirname     = os.path.basename(root)\n",
    "\n",
    "                if new_md5_result not in mnp_dict[\"file_MD5\"]:\n",
    "                    mnp_dict[\"file_MD5\"].append(new_md5_result)\n",
    "                    mnp_dict[\"file_path\"].append(new_path_result)\n",
    "                    mnp_dict[\"file_name\"].append(new_name_result)\n",
    "                    mnp_dict[\"file_dirname\"].append(new_dirname)\n",
    "\n",
    "    return mnp_dict\n",
    "\n",
    "LIST_ARCH = mnp_dict(folder_BJGS_RAW_Path)\n",
    "df_arch = pd.DataFrame.from_dict(LIST_ARCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the dataframe from MetaDrive CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = open(CSV_Metadrive_Path).read().split('\\n')\n",
    "CSV_Metadrive = {}\n",
    "keys = input_file[0].split('''\"\\t\"''')\n",
    "keys[-1] = keys[-1][:-1]\n",
    "input_file.pop(0)\n",
    "\n",
    "for index in keys:\n",
    "    CSV_Metadrive[index.replace('\\ufeff','')] = []\n",
    "\n",
    "for line in input_file:\n",
    "    line = line.split('''\"\\t\"''')\n",
    "    i = 0\n",
    "    for element in line:\n",
    "        CSV_Metadrive[keys[i].replace('\\ufeff','')].append(element)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "df_metadrive = pd.DataFrame.from_dict(CSV_Metadrive)\n",
    "\n",
    "df_metadrive['Name'] = df_metadrive['Name'].str[1:]\n",
    "df_metadrive['Folder'] = df_metadrive['Folder'].str[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name\n",
      "Tipo de Juicio\n",
      "Cuaderno\n",
      "Materia\n",
      "Juzgado\n",
      "Secretaria\n",
      "Núm. Exp.\n",
      "Categoría\n",
      "Tipo de documento\n",
      "Etapa procesal\n",
      "¿Tiene anexos?\n",
      "Documentos anexos\n",
      "Documento relacionado\n",
      "¿Quién lo emite?\n",
      "¿A quién se dirige?\n",
      "Término (días)\n",
      "Folio\n",
      "Description\n",
      "Fecha de promoción\n",
      "Fecha de publicación\n",
      "Fecha de surtimiento de efectos\n",
      "Acuerdo que le recae\n",
      "Día de la notificación\n",
      "Document Link\n",
      "Modified\n",
      "Created\n",
      "Modified By\n",
      "Last viewed by me\n",
      "Owner\n",
      "Folder\n"
     ]
    }
   ],
   "source": [
    "for key, value in CSV_Metadrive.items():\n",
    "    if len(CSV_Metadrive[key]) != 749:\n",
    "        print (key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate dataframe from JSONs (UDT)\n",
    "A. Partir del TXT para corregir los samples de los diferentes JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_jsons(path_dir = folder_UDT_Path):\n",
    "    \n",
    "    return_list_json = []\n",
    "    for root, dirs, files in os.walk(path_dir): \n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                return_list_json.append(os.path.join(root, file))\n",
    "    #return [os.path.join(root, file) for root, dirs, files in os.walk(path_dir) for file in files if file.endswith('.json')]\n",
    "    return return_list_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html import entities\n",
    "from regex import F\n",
    "import unicodedata as ud\n",
    "import string\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nkfd_form = ud.normalize('NFKD', input_str)\n",
    "    return u\"\".join([c for c in nkfd_form if not ud.combining(c)])\n",
    "\n",
    "dict2dump = {\n",
    "    '_id':[], \n",
    "    'document':[], \n",
    "    'file_name':[], \n",
    "    'file_path':[], \n",
    "    'file_dirname':[], \n",
    "    'annotation.entities.0.text':[], \n",
    "    'annotation.entities.0.label':[], \n",
    "    'annotation.entities.0.start':[], \n",
    "    'annotation.entities.0.end':[]}\n",
    "\n",
    "for file in return_jsons():\n",
    "    f = open(file)\n",
    "    \n",
    "        ## Treat file data to dump it in a dictionary suitable for pandas dataframe:\n",
    "    with open(file, 'r') as file:\n",
    "        data = json.load(file)                                                                          # Charge file in data variable\n",
    "        data.pop(\"interface\",404)                                                                       # Delete interface key\n",
    "        data.pop(\"name\",404)                                                                            # Delete name key\n",
    "        for i in data['samples']:                                                                       # For each sample\n",
    "            dict2dump['_id'].append(i['_id'])                                                           # Append _id in the dict\n",
    "            dict2dump['document'].append(i['document'])                                                 # Append document in the dict (complete phrase)\n",
    "            dict2dump['file_name'].append(os.path.basename(file.name))                                  # Append file name from ios_text type\n",
    "            dict2dump['file_path'].append(file.name[:-(1+len(os.path.basename(file.name)))])            # Append path from ios_text type\n",
    "            dict2dump['file_dirname'].append(os.path.basename(file.name[:-(1+len(os.path.basename(file.name)))])) # Same for directory name\n",
    "            if ('annotation' in i):                                                                     # Check if key \"annotation\" exists\n",
    "                if len(i['annotation']['entities']) == 0:                                               # if so, check if it has zero entities\n",
    "                    numannot = 0\n",
    "                    while True:                                                                         # if it has zero entities, we need to populate the appropriate entities columns with NaN\n",
    "                        if f'annotation.entities.{numannot}.text' in dict2dump:                         # Check if the entities column exists\n",
    "                            dict2dump[f'annotation.entities.{numannot}.text'].append('NaN')             # If the entity columns exists, then populate the four columns\n",
    "                            dict2dump[f'annotation.entities.{numannot}.label'].append('NaN')\n",
    "                            dict2dump[f'annotation.entities.{numannot}.start'].append('NaN')\n",
    "                            dict2dump[f'annotation.entities.{numannot}.end'].append('NaN')\n",
    "                            numannot += 1                                                               # Repeat with next entity number\n",
    "                        else:\n",
    "                            break                                                                       # Stop populating if entity number does not exist\n",
    "                \n",
    "                else:                                                                                   # If it has at least one entity\n",
    "                    numannot = 0\n",
    "                    for annot in i['annotation']['entities']:                                           # For each entity in the sample annotation\n",
    "\n",
    "                        if (f'annotation.entities.{numannot}.text' in dict2dump):                       # Check if entity number is in the dict.\n",
    "                            dict2dump[f'annotation.entities.{numannot}.text'].append(annot['text'])     # If so, dump values for the four entities columns\n",
    "                            dict2dump[f'annotation.entities.{numannot}.label'].append(annot['label'])\n",
    "                            dict2dump[f'annotation.entities.{numannot}.start'].append(annot['start'])\n",
    "                            dict2dump[f'annotation.entities.{numannot}.end'].append(annot['end'])\n",
    "\n",
    "                        else:                                                                           # If entity number is not in the dict\n",
    "                            dict2dump[f'annotation.entities.{numannot}.text']= []                       # Create the four columns with the new entity number\n",
    "                            dict2dump[f'annotation.entities.{numannot}.label']= []\n",
    "                            dict2dump[f'annotation.entities.{numannot}.start']= []\n",
    "                            dict2dump[f'annotation.entities.{numannot}.end']= []\n",
    "\n",
    "                            for p in range(len(dict2dump['_id'])-1):                                    # Get the number of rows in a range, minus the last one\n",
    "                                dict2dump[f'annotation.entities.{numannot}.text'].append('NaN')         # For each row, populate the new four new entity columns with NaN\n",
    "                                dict2dump[f'annotation.entities.{numannot}.label'].append('NaN')\n",
    "                                dict2dump[f'annotation.entities.{numannot}.start'].append('NaN')\n",
    "                                dict2dump[f'annotation.entities.{numannot}.end'].append('NaN')\n",
    "                            \n",
    "                            dict2dump[f'annotation.entities.{numannot}.text'].append(annot['text'])     # Append the last row of the four new entity columns\n",
    "                            dict2dump[f'annotation.entities.{numannot}.label'].append(annot['label'])   # with the four value of the entity\n",
    "                            dict2dump[f'annotation.entities.{numannot}.start'].append(annot['start'])\n",
    "                            dict2dump[f'annotation.entities.{numannot}.end'].append(annot['end'])                            \n",
    "                            \n",
    "                        numannot += 1                                                                   # Repeat with next entity number\n",
    "                    while True:\n",
    "                        if f'annotation.entities.{numannot}.text' in dict2dump:                         # Check if other entities column exists\n",
    "                            dict2dump[f'annotation.entities.{numannot}.text'].append('NaN')             # If so, then populate the four columns\n",
    "                            dict2dump[f'annotation.entities.{numannot}.label'].append('NaN')\n",
    "                            dict2dump[f'annotation.entities.{numannot}.start'].append('NaN')\n",
    "                            dict2dump[f'annotation.entities.{numannot}.end'].append('NaN')\n",
    "                            numannot += 1\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "            else:                                                                                       # If key \"annotation\" does not exist\n",
    "                numannot = 0\n",
    "                while True:\n",
    "                    if f'annotation.entities.{numannot}.text' in dict2dump:                             # Check if the entities column exists\n",
    "                        dict2dump[f'annotation.entities.{numannot}.text'].append('NaN')                 # If the entity columns exists, then populate the four columns\n",
    "                        dict2dump[f'annotation.entities.{numannot}.label'].append('NaN')\n",
    "                        dict2dump[f'annotation.entities.{numannot}.start'].append('NaN')\n",
    "                        dict2dump[f'annotation.entities.{numannot}.end'].append('NaN')\n",
    "                        numannot += 1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "df_json_udt = pd.DataFrame.from_dict(dict2dump)\n",
    "df_json_udt['file_dirname'] = df_json_udt['file_dirname'].replace('-', ' ', regex = True)\n",
    "df_arch['file_dirname'] = df_arch['file_dirname'].replace('-', ' ', regex = True)\n",
    "\n",
    "\n",
    "df_arch['file_dirname'] = remove_accents('%&'.join(df_arch['file_dirname'])).split('%&')\n",
    "df_json_udt['file_dirname'] = remove_accents('%&'.join(df_json_udt['file_dirname'])).split('%&')\n",
    "\n",
    "df_arch['file_name'] = remove_accents('%&'.join(df_arch['file_name'])).split('%&')\n",
    "df_json_udt['file_name'] = remove_accents('%&'.join(df_json_udt['file_name'])).split('%&')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Join dataframes (df_json_udt | df_arch | df_metadrive) on file and directory names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Join df_json_udt & df_arch on file and directory names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arch['file_name'] = df_arch['file_name'].replace(\".pdf\",\"\", regex=True)\n",
    "\n",
    "df_json_udt['file_name'] = df_json_udt['file_name'].replace('.udt.json','', regex = True)\n",
    "df_json_udt['file_name'] = df_json_udt['file_name'].replace('.json','', regex = True)\n",
    "\n",
    "\n",
    "df_json_arch = pd.merge(df_json_udt, df_arch, on=['file_name', 'file_dirname'], how='inner')\n",
    "df_json_arch['file_dirname'] = df_json_arch['file_dirname'].replace('Ó','Ó', regex = True)\n",
    "df_json_arch.rename(columns = {'file_path_x':'file_path'}, inplace = True)\n",
    "del df_json_arch['file_path_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Final join with df_metadrive on file and directory names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadrive.rename(columns = {'Name':'file_name'}, inplace = True)\n",
    "df_metadrive.rename(columns = {'Folder':'file_dirname'}, inplace = True)\n",
    "df_metadrive['file_dirname'] = df_metadrive['file_dirname'].replace('-', ' ', regex = True)\n",
    "df_metadrive['file_name'] = df_metadrive['file_name'].replace(\"\\.pdf\",\"\", regex=True)\n",
    "df_metadrive['file_name'] = df_metadrive['file_name'].replace(\"\\.jpeg\",\"\", regex=True)\n",
    "df_metadrive['file_name'] = df_metadrive['file_name'].replace(\"\\.jpg\",\"\", regex=True)\n",
    "df_metadrive['file_name'] = df_metadrive['file_name'].replace(\"\\.png\",\"\", regex=True)\n",
    "df_metadrive['file_name'] = df_metadrive['file_name'].replace(\"\\.doc\",\"\", regex=True)\n",
    "df_metadrive['file_name'] = df_metadrive['file_name'].replace(\"\\.docx\",\"\", regex=True)\n",
    "df_metadrive['file_name'] = df_metadrive['file_name'].replace(\"\\.txt\",\"\", regex=True)\n",
    "df_metadrive['file_dirname'] = df_metadrive['file_dirname'].replace(\"/\",\" \", regex=True)\n",
    "\n",
    "df_metadrive['file_dirname'] = remove_accents('%&'.join(df_metadrive['file_dirname'])).split('%&')\n",
    "df_metadrive['file_name'] = remove_accents('%&'.join(df_metadrive['file_name'])).split('%&')\n",
    "\n",
    "df_final = pd.merge(df_metadrive, df_json_arch, on=['file_name', 'file_dirname'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20131115 - DOCUMENTO 4']\n"
     ]
    }
   ],
   "source": [
    "ctrl_list = []\n",
    "for i in pd.unique(df_json_arch['file_name']):\n",
    "    if i not in pd.unique(df_metadrive['file_name']):\n",
    "        ctrl_list.append(i)\n",
    "\n",
    "print(ctrl_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/philippeprince/Library/CloudStorage/GoogleDrive-pprince@up.edu.mx/Mon Drive/Laboratorio D-Ius/6 - Proyectos del Laboratorio/LLITRA/LLITRA 2021/Work2ETA/1B - Construcción de base de datos consolidada entre Metadrive y Universal Data Tools/1B_consolidate_LLITRA_db.ipynb Cellule 25\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/philippeprince/Library/CloudStorage/GoogleDrive-pprince%40up.edu.mx/Mon%20Drive/Laboratorio%20D-Ius/6%20-%20Proyectos%20del%20Laboratorio/LLITRA/LLITRA%202021/Work2ETA/1B%20-%20Construccio%CC%81n%20de%20base%20de%20datos%20consolidada%20entre%20Metadrive%20y%20Universal%20Data%20Tools/1B_consolidate_LLITRA_db.ipynb#X33sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     dict_iob[\u001b[39m'\u001b[39m\u001b[39mTag\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39mO\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/philippeprince/Library/CloudStorage/GoogleDrive-pprince%40up.edu.mx/Mon%20Drive/Laboratorio%20D-Ius/6%20-%20Proyectos%20del%20Laboratorio/LLITRA/LLITRA%202021/Work2ETA/1B%20-%20Construccio%CC%81n%20de%20base%20de%20datos%20consolidada%20entre%20Metadrive%20y%20Universal%20Data%20Tools/1B_consolidate_LLITRA_db.ipynb#X33sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     dict_iob[\u001b[39m'\u001b[39m\u001b[39mMD5\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(df_final[\u001b[39m'\u001b[39m\u001b[39mfile_MD5\u001b[39m\u001b[39m'\u001b[39m][ind])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/philippeprince/Library/CloudStorage/GoogleDrive-pprince%40up.edu.mx/Mon%20Drive/Laboratorio%20D-Ius/6%20-%20Proyectos%20del%20Laboratorio/LLITRA/LLITRA%202021/Work2ETA/1B%20-%20Construccio%CC%81n%20de%20base%20de%20datos%20consolidada%20entre%20Metadrive%20y%20Universal%20Data%20Tools/1B_consolidate_LLITRA_db.ipynb#X33sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     dict_iob[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(df_final[\u001b[39m'\u001b[39m\u001b[39mTipo de documento\u001b[39m\u001b[39m'\u001b[39m][ind])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/philippeprince/Library/CloudStorage/GoogleDrive-pprince%40up.edu.mx/Mon%20Drive/Laboratorio%20D-Ius/6%20-%20Proyectos%20del%20Laboratorio/LLITRA/LLITRA%202021/Work2ETA/1B%20-%20Construccio%CC%81n%20de%20base%20de%20datos%20consolidada%20entre%20Metadrive%20y%20Universal%20Data%20Tools/1B_consolidate_LLITRA_db.ipynb#X33sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     dict_iob[\u001b[39m'\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(df_final[\u001b[39m'\u001b[39m\u001b[39mCategoría\u001b[39m\u001b[39m'\u001b[39m][ind])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/philippeprince/Library/CloudStorage/GoogleDrive-pprince%40up.edu.mx/Mon%20Drive/Laboratorio%20D-Ius/6%20-%20Proyectos%20del%20Laboratorio/LLITRA/LLITRA%202021/Work2ETA/1B%20-%20Construccio%CC%81n%20de%20base%20de%20datos%20consolidada%20entre%20Metadrive%20y%20Universal%20Data%20Tools/1B_consolidate_LLITRA_db.ipynb#X33sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mcontinue\u001b[39;00m                                                \u001b[39m# and continue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dict_iob ={\n",
    "    'Word':[], \n",
    "    'Tag':[],\n",
    "    'MD5':[],\n",
    "    'type':[],\n",
    "    'category':[]}\n",
    "\n",
    "for ind in df_final.index:\n",
    "    if \"EXP\" in df_final['document'][ind]:\n",
    "        sample = df_final['document'][ind].replace(\".\",\" \")\n",
    "    else:\n",
    "        sample = df_final['document'][ind]\n",
    "\n",
    "    sample = remove_accents(sample)\n",
    "    \n",
    "    if df_final['annotation.entities.0.start'][ind] == 'NaN':   # If there is not any anotation,\n",
    "        for w in sample.split(\" \"):                             # then store the whole sample spliting with space\n",
    "            dict_iob['Word'].append(w)\n",
    "            dict_iob['Tag'].append('O')\n",
    "            dict_iob['MD5'].append(df_final['file_MD5'][ind])\n",
    "            dict_iob['type'].append(df_final['Tipo de documento'][ind])\n",
    "            dict_iob['category'].append(df_final['Categoría'][ind])\n",
    "        continue                                                # and continue\n",
    "\n",
    "    else:\n",
    "\n",
    "        i = 0\n",
    "        annotationlist = {'start':[],'document':[],'label':[],'end':[]}     # Make a dict of whole anotations\n",
    "        while True:\n",
    "            try:\n",
    "                if df_final[f'annotation.entities.{i}.start'][ind] == 'NaN':\n",
    "                    break                \n",
    "                else:\n",
    "                    annotationlist['start'].append(df_final[f'annotation.entities.{i}.start'][ind])\n",
    "                    annotationlist['document'].append(remove_accents(df_final[f'annotation.entities.{i}.text'][ind].replace(\".\",\" \")))\n",
    "                    annotationlist['label'].append(df_final[f'annotation.entities.{i}.label'][ind])\n",
    "                    annotationlist['end'].append(df_final[f'annotation.entities.{i}.end'][ind])\n",
    "                i+=1     \n",
    "            except:\n",
    "                break\n",
    "        \n",
    "        sampleinit = sample[:annotationlist['start'][0]]\n",
    "        for w in sampleinit.split(\" \"):\n",
    "            dict_iob['Word'].append(w)\n",
    "            dict_iob['Tag'].append('O')\n",
    "            dict_iob['MD5'].append(df_final['file_MD5'][ind])\n",
    "            dict_iob['type'].append(df_final['Tipo de documento'][ind])\n",
    "            dict_iob['category'].append(df_final['Categoría'][ind])\n",
    "        \n",
    "        df_annot = pd.DataFrame.from_dict(annotationlist)\n",
    "        for indannot in df_annot.index:\n",
    "            for w in df_annot['document'][indannot].split(\" \"):\n",
    "                dict_iob['Word'].append(w)\n",
    "                if dict_iob['Tag'][-1] == ('I-' + df_annot['label'][indannot]):\n",
    "                    dict_iob['Tag'].append('I-' + df_annot['label'][indannot])\n",
    "                    dict_iob['MD5'].append(df_final['file_MD5'][ind])\n",
    "                    dict_iob['type'].append(df_final['Tipo de documento'][ind])\n",
    "                    dict_iob['category'].append(df_final['Categoría'][ind])\n",
    "                else:\n",
    "                    if (dict_iob['Tag'][-1].startswith(\"B\")) and (dict_iob['Tag'][-1][2:] == df_annot['label'][indannot]):\n",
    "                        dict_iob['Tag'].append('I-' + df_annot['label'][indannot])\n",
    "                        dict_iob['MD5'].append(df_final['file_MD5'][ind])\n",
    "                        dict_iob['type'].append(df_final['Tipo de documento'][ind])\n",
    "                        dict_iob['category'].append(df_final['Categoría'][ind])\n",
    "                    else:\n",
    "                        dict_iob['Tag'].append('B-' + df_annot['label'][indannot])\n",
    "                        dict_iob['MD5'].append(df_final['file_MD5'][ind])\n",
    "                        dict_iob['type'].append(df_final['Tipo de documento'][ind])\n",
    "                        dict_iob['category'].append(df_final['Categoría'][ind])\n",
    "            try:\n",
    "                if df_annot['end'][indannot]+1 == df_annot['start'][indannot+1]:\n",
    "                    continue\n",
    "                else:\n",
    "                    sampleinbetween = sample[df_annot['end'][indannot]:df_annot['start'][indannot+1]]\n",
    "                    for w in sampleinbetween.split(\" \"):\n",
    "                        dict_iob['Word'].append(w)\n",
    "                        dict_iob['Tag'].append('O')\n",
    "                        dict_iob['MD5'].append(df_final['file_MD5'][ind])\n",
    "                        dict_iob['type'].append(df_final['Tipo de documento'][ind])\n",
    "                        dict_iob['category'].append(df_final['Categoría'][ind])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        samplefinal = sample[annotationlist['end'][-1]:]\n",
    "        for w in samplefinal.split(\" \"):\n",
    "            dict_iob['Word'].append(w)\n",
    "            dict_iob['Tag'].append('O')\n",
    "            dict_iob['MD5'].append(df_final['file_MD5'][ind])\n",
    "            dict_iob['type'].append(df_final['Tipo de documento'][ind])\n",
    "            dict_iob['category'].append(df_final['Categoría'][ind])\n",
    "        \n",
    "df_iob = pd.DataFrame.from_dict(dict_iob)  \n",
    "\n",
    "df_iob['Sentence #'] = ''\n",
    "df_iob['Sentence #'][0] = 'Sentence : 0'\n",
    "\n",
    "i = 1\n",
    "for ind in df_iob.index:\n",
    "    if '.' in df_iob['Word'][ind]:\n",
    "        df_iob['Sentence #'][ind+1] = f'Sentence : {i}'\n",
    "        if any(x in df_iob['Word'][ind+1] for x in '''!\"#$%&'()*+,-:;<=>?@[]^_`{|}~—”�''') and len(df_iob['Word'][ind+1])<2:\n",
    "            df_iob['Word'][ind] = ' ' + df_iob['Word'][ind]\n",
    "        i += 1\n",
    "\n",
    "    if any(x in df_iob['Word'][ind] for x in '''!\"#$%&'()*+,-:;<=>?@[]^_`{|}~—”�'''):\n",
    "        for character in '''!\"#$%&'()*+,-:;<=>?@[]^_`{|}~—”�''':\n",
    "            df_iob['Word'][ind] = df_iob['Word'][ind].replace(character, '')\n",
    "    \n",
    "    if df_iob['Sentence #'][ind] == \"\":\n",
    "        df_iob['Sentence #'][ind] = df_iob['Sentence #'][ind -1]\n",
    "\n",
    "raws2drop = []\n",
    "for ind in df_iob.index:\n",
    "    if (df_iob['Word'][ind] == \"\"):\n",
    "        raws2drop.append(ind)\n",
    "df_iob = df_iob.drop(raws2drop)\n",
    "df_iob = df_iob.reset_index(level=None, drop=False, inplace=False, col_level=0, col_fill='')\n",
    "del df_iob['index']\n",
    "\n",
    "for ind in df_iob.index:    \n",
    "    if ind > 0:\n",
    "        if df_iob['Tag'][ind].startswith('I') and df_iob['Tag'][ind-1] == 'O':\n",
    "            df_iob['Tag'][ind] = 'B' + df_iob['Tag'][ind][1:]\n",
    "        if df_iob['Tag'][ind].startswith('B') and df_iob['Tag'][ind][1:] == df_iob['Tag'][ind-1][1:]:\n",
    "            df_iob['Tag'][ind] = 'I' + df_iob['Tag'][ind][1:]\n",
    "        if df_iob['Tag'][ind].startswith('I') and df_iob['Sentence #'][ind-1] != df_iob['Sentence #'][ind]:\n",
    "            df_iob['Tag'][ind] = \"B\" + df_iob['Tag'][ind][1:]\n",
    "\n",
    "first_column = df_iob.pop('Sentence #')\n",
    "df_iob.insert(0, 'Sentence #', first_column)\n",
    "df_iob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iob.to_csv('df_iob.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12 | packaged by conda-forge | (default, Jan 30 2022, 23:33:09) \n[Clang 11.1.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a37aa677fa97143dc02ffad92af644d6e59fb8e41130680c4ad8e633f2ee248"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
